---
output:
  rmarkdown::html_document:
   highlight: pygments
   toc: true
   toc_depth: 3
   fig_width: 5
vignette: >
  %\VignetteIndexEntry{Effectively using the DelayedArray framework as a user to support the analysis of large datasets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding[ut8]{inputenc}
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(
    comment = "#>",
    collapse = TRUE)
```

# Effectively using the DelayedArray framework as a user to support the analysis of large datasets

BioC 2019: 24-27 June

## Instructors

- Peter Hickey (hickey@wehi.edu.au)

## Workshop description

This workshop gives an introductory overview of the DelayedArray framework, which can be used by Bioconductor packages to support the analysis of large datasets.
A *DelayedArray* is like an ordinary array in R, but allows for the data to be in-memory^[Including space-efficient formats like sparse arrays and run length encoded arrays], on-disk in a file, or even hosted on a remote server.
Participants will learn where they might encounter a *DelayedArray* in the wild while using Bioconductor and help them understand the fundamental concepts underlying the framework.
This workshop will be a mixture of lecture with example code and discussion.
Examples will mostly be drawn from the analysis of single-cell RNA-sequencing data, as well as whole-genome bisulfite sequencing and coverage-based assays like ATAC-seq and ChIP-seq.

### Pre-requesites

- Basic knowledge of R syntax.
- Familiarity with common operations on matrices in R, such as `colSums()` and `colMeans()`.
- Some familiarity with S4 objects may be helpful but is not required.
- Some familiarity with single-cell RNA-sequencing may be helpful but is not required.

### Workshop Participation

Students will be able to run code interactively during the workshop.
There will be opportunities throughout the workshop for questions and discussion.

### _R_ / _Bioconductor_ packages used

**TODO:** Once written, go through material and add here.

- DelayedArray
- HDF5Array
- DelayedMatrixStats
- TENxBrainData

### Time outline

| Activity                                        | Time |
|-------------------------------------------------|------|
| Introductory material                           | 10m  |
| First contact                                   | 40m  |
| Package ecosystem                               | 10m  |
| Real world encounters                           | 20m  |
| Workflow tips for DelayedArray-backed analyses  | 20m  |

### Workshop goals and objectives

#### Learning goals

- Learn of existing packages and functions that *use* the DelayedArray framework.
- Develop a high-level understanding of classes and packages that *implement* the DelayedArray framework.
- Become familiar with the fundamental concepts of delayed operations, block-processing, and realization.
- Reason about potential bottlenecks in algorithms operating on *DelayedArray* objects.
- Reason about parallelisation strategies for algorithms operating on *DelayedArray* objects.

#### Learning objectives

- Identify when an object is a *DelayedArray* or one of its derivatives.
- Be able to recognise when it is useful to use a *DelayedArray* instead of an ordinary array or other array-like data structure.
- Learn how to load and save a DelayedArray-backed object.
- Learn how the 'block size' and 'chunking' of the data set affect performance when operating on *DelayedArray* objects.
- Take away some miscellaneous tips and tricks I've learnt over the years when working with DelayedArray-backed objects.

## Introductory material (10 mins)

Data from a high-throughput biological assay, such as single-cell RNA-sequencing (scRNA-seq), will often be summarised as a matrix of counts, where rows correspond to features and columns to samples^[Higher-dimensional arrays may be appropriate for some types of assays.].
Within **Bioconductor**, the *SummarizedExperiment* class is the recommended container for such data, offering a rich interface that tightly links assay measurements to data on the features and the samples.

The _SummarizedExperiment_ class is used to store rectangular arrays of experimental results (_assays_).
Here, each _assay_ is drawn as a matrix but higher-dimensional arrays are also supported.

```{r include = FALSE}
# download current version of SE diagram
se_path <- file.path(tempdir(), "SE.svg")
download.file(
  "https://docs.google.com/feeds/download/drawings/Export?id=1kiC8Qlo1mhSnLDqkGiRNPSo6GWn3C2duBszCFbJCB-g&exportFormat=svg", 
  se_path)
```

`r knitr::include_graphics(se_path)`

Traditionally, the assay data are stored in-memory as an ordinary *array* object^[In R, a *matrix* is just a 2-dimensional *array*]. 
Storing the data in-memory becomes a real pain with the ever-growing size of 'omics datasets.
It is now not uncommon to collect $10,000-100,000,000$ measurements on $100 - 1,000,000$ samples, which would occupy $10-1,000$ gigabytes (GB) if stored in-memory as ordinary R arrays.

### Things DA has enabled (5 mins)

- Nature Neuro
- eGTEx
- 10x analysis on a laptop

## First contact (40 mins)

We will begin with an example using some single-cell RNA-seq data on 1.3 million brain cells from embryonic mice, generated by 10X Genomics.
These data are available in the **TENxBrainData** Bioconductor package.

```{r, message = FALSE}
library(DelayedArray)
# NOTE: This option is for didactic purposes.
setAutoBPPARAM(SerialParam())

# NOTE: The TENxBrainData package loads and attaches the HDF5Array package, 
#       amongst others.
library(TENxBrainData)

# NOTE: This will download the data and may take a little while on the first 
#       run. The result will be cached, however, so subsequent runs avoid 
#       re-downloading the data.
tenx <- TENxBrainData()
```

Let's take a look at the `tenx` object.

```{r}
tenx
```

The data are stored in a *SingleCellExperiment* object, an extension of the *SummarizedExperiment* class.
With data from 1.3 million cells, this is roughly 100,000-times more samples
than a typical bulk RNA-seq data set and would occupy 146 GB in-memory if stored as an ordinary *matrix*.

We might expect that interacting with an object containing this many samples would feel sluggist, but this is not the case.
For example, we can efficiently subset the object.

```{r}
# Take a random sample of 1000 genes and 200000 samples
tenx[sample(nrow(tenx), 1000), sample(ncol(tenx), 200000)]
```

Now, this efficiency could all be down to some trickery through the outer *SingleCellExperiment* class.
To make things clearer, we'll extract the `counts` assay.

```{r}
# NOTE: We'll discuss the use of `withDimnames = FALSE` later in the workshop.
tenx_counts <- assay(tenx, "counts", withDimnames = FALSE)
```

Now, let's do something that would ordinarily be a terrible idea, and something that's frustrated me way too many times: let's "accidentally" print out the entire counts matrix.

```{r}
tenx_counts
```

Hallelujah!
Unlike what you may have experienced when printing out a large matrix, this didn't overwhelm the screen with thousands of lines of output nor did it cause the R session to hang indefinitely.
In fact, this gives us a rather pretty printing of the counts matrix^[You may have seen similar pretty printing with other Bioconductor objects such as *GRanges* and *DataFrame* or with the *data.table* and *tbl* extensions to the *data.frame*. I can't say enough how much I appreciate these thoughtful touches when doing interactive data analysis.].
No need for panicked mashing of `Ctrl-c` or `Esc`. 

We can now clearly see that `tenx_counts` is no ordinary *matrix*.
In fact, it is an *HDF5Matrix*, which is a type of *DelayedArray*^[As with a 2-dimensional *array* in base R being commonly known as a *matrix*, a 2-dimensional *DelayedArray* is also known as a *DelayedMatrix* and a 2-dimensional *HDF5Array* is also known as a *HDF5Matrix*.].

```{r}
class(tenx_counts)
is(tenx_counts, "DelayedArray")
```

The data contained in an *HDF5Matrix* is actually stored on disk in a [Hierarchical Data Format (**HDF5**)](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) file.
Consequently, the `tenx_counts` object takes up very little space in memory.

```{r}
print(object.size(tenx_counts), units = "auto")
```

We can learn more about the internals of the `tenx_counts` object using the `seed()` function^[We will revisit the `chunkdim` component of the *HDF5Matrix* object later in the workshop.].

```{r}
seed(tenx_counts)
```

### Three examples of computing on a DelayedArray

We will now play around with computing on the counts matrix.
To make things slightly easier, we will first subset the data to 1000 samples.

```{r}
tenx_counts_subset <- tenx_counts[, 1:1000]
```

#### Library sizes

Firstly, let's compute the library sizes for this subset of samples.
We can do this using `colSums()`.

```{r}
lib_sizes <- colSums(tenx_counts_subset)
summary(lib_sizes)
```

#### Proportion of cells with non-zero expression for each gene

Secondly, suppose we want to know for each gene the proportion of cells with non-zero expression.
We can do this using `rowSums()` in conjunction with some standard R command.

```{r}
prop_non_zero <- rowSums(tenx_counts_subset > 0) /  ncol(tenx_counts_subset)
summary(prop_non_zero)
```

#### Median expression of each gene

Finally, suppose we want to know the median expression of each gene.
Here, we will quantify expression as counts per million (CPM) using library size normalization.

```{r}
cpm <- t(t(1e6 * tenx_counts_subset) / lib_sizes)
cpm
```

We can then compute the median expression of each gene using `DelayedMatrixStats::rowMedians()`.

```{r}
library(DelayedMatrixStats)
median_expression <- rowMedians(cpm)
summary(median_expression)
```

#### Summary

These 3 examples highlight the power of the DelayedArray framework
Recall that the data in these examples live on disk in an HDF5 file, yet we interacted with `tenx_counts_subset` and computed on it much as we would if the data were in-memory as an ordinary matrix.
Also note that all 3 examples returned ordinary R vectors.

```{r}
class(lib_sizes)
class(prop_non_zero)
class(cpm)
```

To do so, we made (implicit) use of the three fundamental concepts of the DelayedArray framework:

1. Delayed operations
2. Block-processing
3. Realization

We'll now discuss each of these in turn.

### Delayed operations

Taking a careful look at `tenx_counts_subset`, we see that it is a *DelayedMatrix* rather than an *HDF5Matrix*.

```{r}
tenx_counts_subset
```

The subsetting operation has 'degraded' the `tenx_counts_subset` object to a *DelayedMatrix*.

```{r}
is(tenx_counts_subset, "HDF5Matrix")
is(tenx_counts_subset, "DelayedMatrix")
```

The `showtree()` function can help us see what changed.

```{r}
showtree(tenx_counts)
showtree(tenx_counts_subset)
```

The subsetting operation has been registered in what is termed a 'delayed operation'.
Registering a delayed operation does not modify the underlying data.
Instead, the operation is recorded and only performed when the *DelayedArray* object is 'realized'.
Realization of a *DelayedArray* triggers the execution of the delayed operations carried by the object and returns the result as an ordinary *array*.

This allows us to chain together multiple operations and only perform them as required.
Here is a contrived example.

```{r}
# Add 1 to every element (a delayed op).
x <- tenx_counts_subset + 1L
showtree(x)

# Compute log of every element (another delayed op).
lx <- log(x)
showtree(lx)

# Transpose the result (another delayed op).
tlx <- t(lx)
showtree(tlx)

# Realize a subset of the data as an ordinary matrix.
as.array(tlx[1:5, 1:10])
```

Many common operations can be registered as delayed operations.
Here are some examples^[The technical names of each type of delayed operation are not important.].
Notice that in each case the result is 'degraded' to a *DelayedMatrix*^[The [No-op] example is the obvious exception].

#### DelayedSubset 

```{r}
val <- tenx_counts[, 1:100]
val
showtree(val)
```

#### DelayedAperm

```{r}
val <- t(tenx_counts)
val
showtree(val)
```

#### DelayedUnaryIsoOp

```{r}
val <- tenx_counts + 1L
val
showtree(val)
val <- tenx_counts + 1:2
val
showtree(val)
```

#### DelayedSubassign

```{r}
# NOTE: Be careful with delayed subassignment. You can end up with objects that 
#       are surprisingly large in-memory.
tmp <- tenx_counts
tmp[, 1] <- 100
tmp
showtree(tmp)
```

#### DelayedDimnames

```{r}
tmp <- tenx_counts
rownames(tmp) <- paste0("R", seq_len(nrow(tmp)))
tmp
showtree(tmp)
```

#### DelayedNaryIsoOp

```{r}
val <- tenx_counts + tenx_counts
val
showtree(val)
```

#### DelayedAbind

```{r}
val <- cbind(tenx_counts, tenx_counts)
val
showtree(val)
```

#### No-op

The DelayedArray framework is smart enough to recognise that some combinations of operations are 'no-ops'.

```{r}
val <- t(t(tenx_counts))
val
showtree(val)
```

But it can be fooled.

```{r}
# NOTE: This is a no-op but DelayedArray doesn't recognise it as one.
val <- tenx_counts + 0L
val
showtree(val)
```

### Block-processing

In [Library sizes], we needed to compute the column sums of `tenx_counts_subset`, whose data live on disk in an HDF5 file.
One way of achieving this would be to load the entire data set into memory as an ordinary matrix and then run `base::colSums()`^[Here, I use the 'namespaced function' notation for didactic purposes to distinguish `base::colSums()` from the S4 generic `colSums()` and its associated methods. When writing code, however, it is generally not necessary to use the `package::function()` notation, except for all the times that it is ...].

```{r}
lib_sizes_in_mem <- colSums(as.array(tenx_counts_subset))
# Check we get the same result as before.
identical(lib_sizes_in_mem, lib_sizes)
```

Here the data at `r print(object.size(as.array(tenx_counts_subset)), units = "auto")` are small enough to load into memory, but what if that's not the case^[For example, recall that the `tenx_counts` data would occupy > 146 GB in memory.]?
One way you might think to do this is to loop over the columns of the matrix, load that column into memory, and compute it's sum^[In the following code example we could replace `as.array(colSums(tenx_counts_subset[, j, drop = FALSE]))` with `sum(tenx_counts_subset[, j, drop = TRUE])` or, more simply, `sum(tenx_counts_subset[, j])`. This is because subsetting a *DelayedArray* returns an ordinary vector when `drop = TRUE` and the result has only one dimension; see 'Subsetting' in `help("DelayedArray", package = "DelayedArray")`.]

```{r}
lib_sizes_loop_over_cols <- vector("numeric", ncol(tenx_counts_subset))
for (j in seq_len(ncol(tenx_counts_subset))) {
  # A simple progress report.
  if (j %% 100 == 0) message("Processed ", j, "/", ncol(tenx_counts_subset))
  lib_sizes_loop_over_cols[j] <- colSums(
    as.array(tenx_counts_subset[, j, drop = FALSE]))
}
# Check we get the same result as before.
identical(lib_sizes_loop_over_cols, lib_sizes)
```

But what if you can't load even a single column into memory^[This may seem artificial, but suppose we were dealing with a very 'tall' matrix.]?
We might loop over the columns of the matrix, partition each column, load each partition, compute its sum, and then compute the sum of the partition sums.

```{r}
# NOTE: Here we will partition each column into two equal-sized subsets.
lib_sizes_loop_over_cols_subset <- vector("numeric", ncol(tenx_counts_subset))
tmp_colsums <- vector("numeric", 2)
nr <- nrow(tenx_counts_subset)

for (j in seq_len(ncol(tenx_counts_subset))) {
  # A simple progress report.
  if (j %% 100 == 0) message("Processed ", j, "/", ncol(tenx_counts_subset))
  for (i in 1:2) {
    i1 <- (i - 1) * nr / 2 + 1
    i2 <- i * nr / 2
    tmp_colsums[i] <- colSums(
      as.array(tenx_counts_subset[seq(i1, i2), j, drop = FALSE]))
  }
  lib_sizes_loop_over_cols_subset[j] <- sum(tmp_colsums)
}
# Check we get the same result as before.
identical(lib_sizes_loop_over_cols_subset, lib_sizes)
```

Hopefully you can now begin to see the general pattern, a strategy which the **DelayedArray** package calls 'block-processing':

1. Load a 'block' of the data into memory.
2. Compute a summary statistic.
3. Combine the block-level statistics in an appropriate way to get the final result.

#### Block-processing illustrated

Some examples of block-processing are illustrated in the following figures:

`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide1.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide2.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide3.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide4.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide5.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide6.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide7.png"))`

#### Block-processed column sums

When we run `colSums(tenx_counts_subset`) we are using a block-processed version of column sums, specifically the `colSums,DelayedMatrix-method` implemented in the **DelayedArray** package.

```{r}
# Turn on progress reports from DelayedArray's block-processing routines.
DelayedArray:::set_verbose_block_processing(TRUE)
head(colSums(tenx_counts_subset))
```

#### More examples of block-processed operations in **DelayedArray**

Some of the most useful functions in the **DelayedArray** package implement common operations on a *DelayedMatrix* using block-processing.
These include the following row and column summarization methods:

- `rowSums()`
- `colSums()`
- `rowMeans()`
- `colMeans()`
- `rowMaxs()`
- `colMaxs()`
- `rowMins()`
- `colMins()`
- `rowRanges()`
- `colRanges()`

Two useful but lesser known functions use block-processing to compute column / row sums of a *DelayedMatrix* based on a grouping variable:

- `rowsum()`
- `colsum()`

```{r}
# Compute separate library sizes for mitochondrial and non-mitochondrial genes.
is_mito <- grepl("^mt-", rowData(tenx)$Symbol)
summary(is_mito)
lib_sizes_grouped <- rowsum(tenx_counts_subset, group = is_mito)
head(t(lib_sizes_grouped))
```

Finally, matrix multiplication is implemented as a block-processed operation.

```{r}
# This is mathematically equivalent to rowSums(tenx_counts_subset).
tenx_counts_subset %*% matrix(1, nrow = ncol(tenx_counts_subset))
```

#### DelayedMatrixStats

We've already seen the [**DelayedMatrixStats**](http://bioconductor.org/packages/DelayedMatrixStats/) package in action back when computing the [Median expression of each gene].
**DelayedMatrixStats** is a port of the [**matrixStats**](https://CRAN.R-project.org/package=matrixStats) API for use with *DelayedMatrix* objects.
It provides [more than 70 functions](https://github.com/PeteHaitch/DelayedMatrixStats#api-coverage) that apply to rows and columns of *DelayedMatrix* objects

#### General block-processing

As we have seen, many common block-processing operations on a *DelayedMatrix* have already been implemented in **DelayedArray** or are provided by **DelayedMatrixStats**.
Nonetheless, there may be times you need to implement your own algorithm using block-processing.
The documentation on this topic is a little sparse, but some details can be found in `help("block_processing", "DelayedArray")` and `help("ArrayGrid", "DelayedArray")` or by reading the source code of the aforementioned packages.
Briefly, to perform block-processing requires that you:

1. Set up an *ArrayGrid* over your *DelayedArray*. This specifies the 'block' structure that will be traversed when processing the *DelayedArray*.
2. Iterate over the *DelayedArray* via the *ArrayGrid*
  a. Read each block of data into memory as an ordinary array using `read_block()`.
  b. Compute the statistic for that block
3. Appropriately combine the block-level statistics to get your final result.

The `blockApply()` and `blockReduce()` functions can help facilitate steps 1-3 , even incorporating parallelization via the [**BiocParallel**](http://bioconductor.org/packages/BiocParallel/) package.

### Realization

To *realize* a *DelayedArray* object is to trigger execution of the delayed operations carried by the object and return the result as an ordinary array,
One way to achieve this is to call `as.array()` on it.

```{r}
tenx_counts_subset_realized <- as.array(tenx_counts_subset)
tenx_counts_subset_realized[1:10, 1:10]
```

However, this realizes the full object at once in memory which could require too much memory if the object is big^[In the above example it's safe because `tenx_counts_subset_realized` only requires `r print(object.size(tenx_counts_subset_realized), units = "auto")` of memory.]
A large *DelayedArray* object is preferrably realized on disk, which is most commonly an HDF5 file.

#### Realizing to an HDF5 file

Realizing to an HDF5 file requires that the **HDF5Array** package is installed^[The **TENxBrainData** package depends upon **HDF5Array**, so it is already installed. If you've been following this workflow then you've also already attached it to the search path by running `library(TENxBrainData)`.]

```{r}
tenx_counts_subset_hdf5 <- writeHDF5Array(tenx_counts_subset)
tenx_counts_subset_hdf5

# Alterntivaly, we could 'coerce' the result to be an HDF5Array.
as(tenx_counts_subset, "HDF5Array")
```

Notice that this process of realization used block-processing, which avoids loading the entire dataset entire memory.
Also notice that the result of realization is here returned as an *HDF5Matrix*, which no longer carries around the delayed operations.

```{r}
class(tenx_counts_subset)
class(tenx_counts_subset_hdf5)

showtree(tenx_counts_subset)
showtree(tenx_counts_subset_hdf5)
```

Used like this, `writeHDF5Array()` and `as(..., "HDF5Array")` will write their results to a file in *HDF5 dump directory*, a dumping ground for automatically created HDF5 datasets.
We can see a log of the operations that have written to the HDF5 dump directory using `showHDF5DumpLog()`.

```{r}
showHDF5DumpLog()
```

Often, however, we will want full control of where and how the data are written to the HDF5 file^[We'll discuss why you might want this in the second half of the workshop.] and the `writeHDF5Array()` function gives you full control over this and more.

```{r}
# Write the data to a user-specified HDF5 file using maximum compression and 
# 'chunking' along the columns.
my_hdf5_file <- tempfile(fileext = ".h5")
tenx_counts_subset_my_file_hdf5 <- writeHDF5Array(
  tenx_counts_subset,
  filepath = my_hdf5_file,
  chunkdim = c(nrow(tenx_counts_subset), 1),
  level = 9)

# Compare `tenx_counts_subset_hdf5` to `tenx_counts_subset_my_file_hdf5`
seed(tenx_counts_subset_hdf5)
seed(tenx_counts_subset_my_file_hdf5)
```

#### Realization backends

We've now seen that we can realize to an HDF5 file.
This is called as the HDF5Array 'realization backend' and is implemented in the **HDF5Array** package.
There are a few other realization backends to be aware of.

```{r}
supportedRealizationBackends()
```

There is also the `NULL` backend, which means the data are realized in memory as an ordinary *array* and then wrapped in a *DelayedArray*.
This is the default realization backend upon loading/attaching the **DelayedArray** package.

```{r}
getRealizationBackend()
```

The default realization backend can be altered with `setRealizationBackend()`.

```{r}
setRealizationBackend("HDF5Array")
getRealizationBackend()
setRealizationBackend(NULL)
getRealizationBackend()
```

It can be important to know what your current realization backend is because it will be used implicitly by some functions.
For example, matrix multiplication that involves a *DelayedMatrix* uses the current realization backend.

```{r}
setRealizationBackend("HDF5Array")
tenx_counts_subset %*% matrix(1, nrow = ncol(tenx_counts_subset))

setRealizationBackend(NULL)
tenx_counts_subset %*% matrix(1, nrow = ncol(tenx_counts_subset))
```

#### The `realize()` function

We've seen that we can realize to the HDF5Array backend using `writeHDF5Array(tenx_counts_subset)` and `as(tenx_counts_subset, "HDF5Array")`.
A third way of realizing a *DelayedArray* to an HDF5 file is with the `realize()` function.

```{r}
realize(tenx_counts_subset, BACKEND = "HDF5Array")
```

So why might you use `realize()` instead of these other options?
Because it allows us to easily switch out the realization backend and will defer to the current realization backend if none is supplied.

```{r}
# Realize to the current realization backend.
getRealizationBackend()
realize(tenx_counts_subset)

# Realize as an RleArray.
setRealizationBackend("RleArray")
tenx_counts_subset_rlearray <- realize(tenx_counts_subset)
tenx_counts_subset_rlearray

# Switch back to the default backend.
setRealizationBackend(NULL)
```

This is probably most useful when writing package code so that you can allow the user control over the realization backend.

## Package ecosystem (10 mins)

- Key user-facing packages
- Key developer-facing packages

## Real world encounters with DelayedArray (20 mins)

- Take a 10x data set in TENxMatrix format.

- Feature some of the cools things Aaron has been doing with DA
  - DeferredMatrix
  - LowRankMatrix
  - beachmat
  - Deferred PCA
  
- RleArray and GPos

## Workflow for DelayedArray-backed analyses (20 mins)

- Saving and loading DA- and HDF5A-backed objects
- Writing functions that 'consume' DAs
- Avoid subsetting too much
- Avoid random access patterns
- Process, save, analyse
- How block size and chunking affect performance
- Parallelisation strategies that work and suck
- Easy wins when using DA
- Debugging
- `withDimnames = FALSE`


### Block-processing performance

Here, this strategy (the 'load it all into memory' strategy) works and is in fact faster and requires less memory allocated^[Total memory allocated is not the same as peak-memory used. For example, a process could allocate 1000 MB of memory as 100 x 10 MB blocks and have a peak memory usage of 10 MB or in 1 x 1000 MB block and have a peak memory usage of 1000 MB. Often it is peak memory we care about, but this is tricky to measure.] than running `colSums(tenx_counts_subset`)`!

```{r}
bench::mark(
  colSums(as.array(tenx_counts_subset)),
  colSums(tenx_counts_subset),
  min_iterations = 10)
```

```{r, echo = FALSE}
knitr::knit_exit()
```

## TODOs

- [ ] Update block-processing figures because code has changed. 
- [ ] Find a 10x data set in TENxData format
- [x] Perhaps use the TENxBrainData20k() data instead.
  - Nah, full data should be fine (but have posted https://github.com/Bioconductor/BiocWorkshops2019/issues/31)
- [x] Separate code blocks where an option to improve clarity.
